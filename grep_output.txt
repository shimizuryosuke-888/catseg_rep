train_net_1001.py:95:            output = output["sem_seg"].argmax(dim=0).to(self._cpu_device)
train_net_0909_single_gpu.py:84:            output = output["sem_seg"].argmax(dim=0).to(self._cpu_device)
demo/visualizer.py:56:                panoptic_seg.to(self.cpu_device), segments_info
demo/visualizer.py:64:                instances = predictions["instances"].to(self.cpu_device)
demo/visualizer.py:93:                    frame, panoptic_seg.to(self.cpu_device), segments_info
demo/visualizer.py:96:                predictions = predictions["instances"].to(self.cpu_device)
demo/visualizer.py:100:                    frame, predictions["sem_seg"].argmax(dim=0).to(self.cpu_device)
demo/predictor.py:56:                panoptic_seg.to(self.cpu_device), segments_info
demo/predictor.py:61:                    predictions["sem_seg"].argmax(dim=0).to(self.cpu_device),
demo/predictor.py:65:                instances = predictions["instances"].to(self.cpu_device)
demo/predictor.py:94:                    frame, panoptic_seg.to(self.cpu_device), segments_info
demo/predictor.py:97:                predictions = predictions["instances"].to(self.cpu_device)
demo/predictor.py:101:                    frame, predictions["sem_seg"].argmax(dim=0).to(self.cpu_device)
train_net_0909_backup.py:83:            output = output["sem_seg"].argmax(dim=0).to(self._cpu_device)
train_net.py:58:            output = output["sem_seg"].argmax(dim=0).to(self._cpu_device)
train_net_0909.py:83:            output = output["sem_seg"].argmax(dim=0).to(self._cpu_device)
plain_train_net.py:113:            output = output["sem_seg"].argmax(dim=0).to(self._cpu_device)
segment-anything/scripts/amg.py:198:    _ = sam.to(device=args.device)
segment-anything/scripts/export_onnx_model.py:171:    return tensor.cpu().numpy()
segment-anything/demo/README.md:35:sam.to(device='cuda')
segment-anything/demo/README.md:44:image_embedding = predictor.get_image_embedding().cpu().numpy()
cat_seg/cat_seg_model_0911_Wrapper_review.py:141:        images = [x["image"].to(self.device) for x in batched_inputs]
cat_seg/cat_seg_model_0911_Wrapper_review.py:165:            targets = torch.stack([x["sem_seg"].to(self.device) for x in batched_inputs], dim=0)
cat_seg/cat_seg_model_0911_Wrapper_review.py:193:        images = [x["image"].to(self.device, dtype=torch.float32) for x in batched_inputs]
cat_seg/cat_seg_model.py:137:        images = [x["image"].to(self.device) for x in batched_inputs]
cat_seg/cat_seg_model.py:161:            targets = torch.stack([x["sem_seg"].to(self.device) for x in batched_inputs], dim=0)
cat_seg/cat_seg_model.py:189:        images = [x["image"].to(self.device, dtype=torch.float32) for x in batched_inputs]
cat_seg/cat_seg_model_0910.py:141:        images = [x["image"].to(self.device) for x in batched_inputs]
cat_seg/cat_seg_model_0910.py:165:            targets = torch.stack([x["sem_seg"].to(self.device) for x in batched_inputs], dim=0)
cat_seg/cat_seg_model_0910.py:193:        images = [x["image"].to(self.device, dtype=torch.float32) for x in batched_inputs]
cat_seg/third_party/clip.py:85:        model = build_model(model.state_dict(), prompt_depth, prompt_length).to(device)
cat_seg/third_party/clip.py:89:    device_holder = torch.jit.trace(lambda: torch.ones([]).to(torch.device(device)), example_inputs=[])
cat_seg/third_party/clip.py:150:        model = build_model(model.state_dict()).to(device)
cat_seg/third_party/clip.py:154:    device_holder = torch.jit.trace(lambda: torch.ones([]).to(torch.device(device)), example_inputs=[])
cat_seg/third_party/model_vpt.py:69:        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC
cat_seg/third_party/model_vpt.py:200:        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None
cat_seg/third_party/model_vpt.py:283:        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]
cat_seg/third_party/model_vpt.py:286:            x = x + self.resized_pos_embed(self.input_resolution, x.shape[1]).to(x.dtype)
cat_seg/third_party/model_vpt.py:288:            x = x + self.positional_embedding.to(x.dtype)
cat_seg/third_party/model.py:70:        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC
cat_seg/third_party/model.py:185:        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None
cat_seg/third_party/model.py:246:        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]
cat_seg/third_party/model.py:249:            x = x + self.resized_pos_embed(self.input_resolution, x.shape[1]).to(x.dtype)
cat_seg/third_party/model.py:251:            x = x + self.positional_embedding.to(x.dtype)
cat_seg/cat_seg_model_0917_Transformer_review.py:142:        images = [x["image"].to(self.device) for x in batched_inputs]
cat_seg/cat_seg_model_0917_Transformer_review.py:167:            targets = torch.stack([x["sem_seg"].to(self.device) for x in batched_inputs], dim=0)
cat_seg/cat_seg_model_0917_Transformer_review.py:195:        images = [x["image"].to(self.device, dtype=torch.float32) for x in batched_inputs]
cat_seg/utils/misc.py:32:        cast_tensor = self.tensors.to(device)
cat_seg/utils/misc.py:36:            cast_mask = mask.to(device)
cat_seg/utils/misc.py:80:            torch.stack([img.shape[i] for img in tensor_list]).to(torch.float32)
cat_seg/utils/misc.py:81:        ).to(torch.int64)
cat_seg/utils/misc.py:98:        padded_masks.append(padded_mask.to(torch.bool))
cat_seg/cat_seg_model_0909.py:141:        images = [x["image"].to(self.device) for x in batched_inputs]
cat_seg/cat_seg_model_0909.py:165:            targets = torch.stack([x["sem_seg"].to(self.device) for x in batched_inputs], dim=0)
cat_seg/cat_seg_model_0909.py:193:        images = [x["image"].to(self.device, dtype=torch.float32) for x in batched_inputs]
cat_seg/utils/prompt_viz.py:24:    x = img_t.detach().float().clamp(0, 255).cpu().numpy()
cat_seg/utils/prompt_viz.py:34:    x = img_t.detach().float().cpu()
cat_seg/utils/prompt_viz.py:157:        img_t = images[b].to(torch.float32)  # [3,H,W]
cat_seg/utils/prompt_viz.py:206:            m_up = F.interpolate(m, size=(H, W), mode="nearest")[0, 0].cpu().numpy()  # 0/1
cat_seg/utils/prompt_viz.py:214:            pts = coords_img[b, cid].cpu().numpy()  # [K,2] (x,y)
cat_seg/utils/corr_heatmap_viz.py:30:    x = img_t.detach().float().cpu()
cat_seg/utils/corr_heatmap_viz.py:184:            x = img_t.detach().float().cpu()
cat_seg/utils/corr_heatmap_viz.py:224:        t = t.detach().float().cpu()  # (B,Ncls,H_emb,W_emb,C)
cat_seg/utils/corr_heatmap_viz.py:284:    corr = corr_grid.detach().float().cpu()  # (B,H_emb,W_emb,Ncls)
cat_seg/modeling/transformer/cat_seg_predictor_0909.py:186:                texts = self.tokenizer(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0909.py:188:                texts = clip.tokenize(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0909.py:196:        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0909.py:212:                    texts = self.tokenizer(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0909.py:214:                    texts = clip.tokenize(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor.py:174:                texts = self.tokenizer(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor.py:176:                texts = clip.tokenize(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor.py:184:        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()
cat_seg/modeling/transformer/cat_seg_predictor.py:200:                    texts = self.tokenizer(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor.py:202:                    texts = clip.tokenize(texts).cuda()
cat_seg/modeling/transformer/model_0917_Transformer_review_miou_bug.py:585:        self.sam_block = sam_block.build_sam_block().to(self.device)
cat_seg/modeling/transformer/model_0917_Transformer_review_miou_bug.py:595:        self.conv1 = nn.Conv2d(prompt_channel, hidden_dim, kernel_size=1).to(self.device)
cat_seg/modeling/transformer/model_0917_Transformer_review_miou_bug.py:600:        self.sam_prompts_conv = nn.Conv2d(self.k_pts*256, 256, kernel_size=1).to(self.device)
cat_seg/modeling/transformer/model_0917_Transformer_review_miou_bug.py:644:        image_pe = self.sam_block.prompt_encoder.get_dense_pe().to(self.device)  # (1,256,?,?)
cat_seg/modeling/transformer/model_0917_Transformer_review_miou_bug.py:655:            coords_b = coords[b].reshape(Nk, 1, 2).to(self.device)                  # (Nk,1,2)
cat_seg/modeling/transformer/model_0917_Transformer_review_miou_bug.py:656:            labels_b = labels[b].reshape(Nk, 1).to(self.device)                     # (Nk,1)
cat_seg/modeling/transformer/model_0917_Transformer_review_miou_bug.py:657:            masks_b  = region_masks[b].reshape(Nk, 1, H_emb_reg, W_emb_reg).to(self.device)
cat_seg/modeling/transformer/model_0929_c_conv_3x3.py:585:        self.sam_block = sam_block.build_sam_block().to(self.device)
cat_seg/modeling/transformer/model_0929_c_conv_3x3.py:595:        self.conv1 = nn.Conv2d(prompt_channel, hidden_dim, kernel_size=1).to(self.device)
cat_seg/modeling/transformer/model_0929_c_conv_3x3.py:600:        self.sam_prompts_conv = nn.Conv2d(self.k_pts*256, 256, kernel_size=1).to(self.device)
cat_seg/modeling/transformer/model_0929_c_conv_3x3.py:642:        image_pe = self.sam_block.prompt_encoder.get_dense_pe().to(self.device)  # (1,256,?,?)
cat_seg/modeling/transformer/model_0929_c_conv_3x3.py:653:            coords_b = coords[b].reshape(Nk, 1, 2).to(self.device)                  # (Nk,1,2)
cat_seg/modeling/transformer/model_0929_c_conv_3x3.py:654:            labels_b = labels[b].reshape(Nk, 1).to(self.device)                     # (Nk,1)
cat_seg/modeling/transformer/model_0929_c_conv_3x3.py:655:            masks_b  = region_masks[b].reshape(Nk, 1, H_emb_reg, W_emb_reg).to(self.device)
cat_seg/modeling/transformer/model_0910_Swin_review.py:551:        self.sam_block = sam_block.build_sam_block().to(self.device)
cat_seg/modeling/transformer/model_0910_Swin_review.py:595:        image_pe = self.sam_block.prompt_encoder.get_dense_pe().to(self.device)  # (1,256,?,?)
cat_seg/modeling/transformer/model_0910_Swin_review.py:606:            coords_b = coords[b].reshape(Nk, 1, 2).to(self.device)                  # (Nk,1,2)
cat_seg/modeling/transformer/model_0910_Swin_review.py:607:            labels_b = labels[b].reshape(Nk, 1).to(self.device)                     # (Nk,1)
cat_seg/modeling/transformer/model_0910_Swin_review.py:608:            masks_b  = region_masks[b].reshape(Nk, 1, H_emb_reg, W_emb_reg).to(self.device)
cat_seg/modeling/transformer/cat_seg_predictor_0911_Wrapper_review.py:187:                texts = self.tokenizer(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0911_Wrapper_review.py:189:                texts = clip.tokenize(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0911_Wrapper_review.py:197:        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0911_Wrapper_review.py:213:                    texts = self.tokenizer(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0911_Wrapper_review.py:215:                    texts = clip.tokenize(texts).cuda()
cat_seg/modeling/transformer/model_0929_c_conv_3x3_device_delete.py:585:        self.sam_block = sam_block.build_sam_block().to(self.device)
cat_seg/modeling/transformer/model_0929_c_conv_3x3_device_delete.py:595:        self.conv1 = nn.Conv2d(prompt_channel, hidden_dim, kernel_size=1).to(self.device)
cat_seg/modeling/transformer/model_0929_c_conv_3x3_device_delete.py:600:        self.sam_prompts_conv = nn.Conv2d(self.k_pts*256, 256, kernel_size=1).to(self.device)
cat_seg/modeling/transformer/model_0929_c_conv_3x3_device_delete.py:642:        image_pe = self.sam_block.prompt_encoder.get_dense_pe().to(self.device)  # (1,256,?,?)
cat_seg/modeling/transformer/model_0929_c_conv_3x3_device_delete.py:653:            coords_b = coords[b].reshape(Nk, 1, 2).to(self.device)                  # (Nk,1,2)
cat_seg/modeling/transformer/model_0929_c_conv_3x3_device_delete.py:654:            labels_b = labels[b].reshape(Nk, 1).to(self.device)                     # (Nk,1)
cat_seg/modeling/transformer/model_0929_c_conv_3x3_device_delete.py:655:            masks_b  = region_masks[b].reshape(Nk, 1, H_emb_reg, W_emb_reg).to(self.device)
cat_seg/modeling/transformer/model_0929_c_conv_3x3_backup_mean.py:585:        self.sam_block = sam_block.build_sam_block().to(self.device)
cat_seg/modeling/transformer/model_0929_c_conv_3x3_backup_mean.py:595:        self.conv1 = nn.Conv2d(prompt_channel, hidden_dim, kernel_size=1).to(self.device)
cat_seg/modeling/transformer/model_0929_c_conv_3x3_backup_mean.py:600:        self.sam_prompts_conv = nn.Conv2d(self.k_pts*256, 256, kernel_size=1).to(self.device)
cat_seg/modeling/transformer/model_0929_c_conv_3x3_backup_mean.py:602:        # self.c_conv = nn.Conv2d(in_channels=hidden_dim, out_channels=1, kernel_size=3, padding=1, bias=True).to(self.device)
cat_seg/modeling/transformer/model_0929_c_conv_3x3_backup_mean.py:642:        image_pe = self.sam_block.prompt_encoder.get_dense_pe().to(self.device)  # (1,256,?,?)
cat_seg/modeling/transformer/model_0929_c_conv_3x3_backup_mean.py:653:            coords_b = coords[b].reshape(Nk, 1, 2).to(self.device)                  # (Nk,1,2)
cat_seg/modeling/transformer/model_0929_c_conv_3x3_backup_mean.py:654:            labels_b = labels[b].reshape(Nk, 1).to(self.device)                     # (Nk,1)
cat_seg/modeling/transformer/model_0929_c_conv_3x3_backup_mean.py:655:            masks_b  = region_masks[b].reshape(Nk, 1, H_emb_reg, W_emb_reg).to(self.device)
cat_seg/modeling/transformer/model_0917_Transformer_review_backup.py:585:        self.sam_block = sam_block.build_sam_block().to(self.device)
cat_seg/modeling/transformer/model_0917_Transformer_review_backup.py:595:        self.conv1 = nn.Conv2d(prompt_channel, hidden_dim, kernel_size=1).to(self.device)
cat_seg/modeling/transformer/model_0917_Transformer_review_backup.py:600:        self.sam_prompts_conv = nn.Conv2d(self.k_pts*256, 256, kernel_size=1).to(self.device)
cat_seg/modeling/transformer/model_0917_Transformer_review_backup.py:641:        image_pe = self.sam_block.prompt_encoder.get_dense_pe().to(self.device)  # (1,256,?,?)
cat_seg/modeling/transformer/model_0917_Transformer_review_backup.py:652:            coords_b = coords[b].reshape(Nk, 1, 2).to(self.device)                  # (Nk,1,2)
cat_seg/modeling/transformer/model_0917_Transformer_review_backup.py:653:            labels_b = labels[b].reshape(Nk, 1).to(self.device)                     # (Nk,1)
cat_seg/modeling/transformer/model_0917_Transformer_review_backup.py:654:            masks_b  = region_masks[b].reshape(Nk, 1, H_emb_reg, W_emb_reg).to(self.device)
cat_seg/modeling/transformer/model_0929c_conv_3x3_miou_bug.py:585:        self.sam_block = sam_block.build_sam_block().to(self.device)
cat_seg/modeling/transformer/model_0929c_conv_3x3_miou_bug.py:595:        self.conv1 = nn.Conv2d(prompt_channel, hidden_dim, kernel_size=1).to(self.device)
cat_seg/modeling/transformer/model_0929c_conv_3x3_miou_bug.py:600:        self.sam_prompts_conv = nn.Conv2d(self.k_pts*256, 256, kernel_size=1).to(self.device)
cat_seg/modeling/transformer/model_0929c_conv_3x3_miou_bug.py:602:        self.c_conv = nn.Conv2d(in_channels=hidden_dim, out_channels=1, kernel_size=3, padding=1, bias=True).to(self.device)
cat_seg/modeling/transformer/model_0929c_conv_3x3_miou_bug.py:642:        image_pe = self.sam_block.prompt_encoder.get_dense_pe().to(self.device)  # (1,256,?,?)
cat_seg/modeling/transformer/model_0929c_conv_3x3_miou_bug.py:653:            coords_b = coords[b].reshape(Nk, 1, 2).to(self.device)                  # (Nk,1,2)
cat_seg/modeling/transformer/model_0929c_conv_3x3_miou_bug.py:654:            labels_b = labels[b].reshape(Nk, 1).to(self.device)                     # (Nk,1)
cat_seg/modeling/transformer/model_0929c_conv_3x3_miou_bug.py:655:            masks_b  = region_masks[b].reshape(Nk, 1, H_emb_reg, W_emb_reg).to(self.device)
cat_seg/modeling/transformer/cat_seg_predictor_0917_Transformer_review.py:190:                texts = self.tokenizer(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0917_Transformer_review.py:192:                texts = clip.tokenize(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0917_Transformer_review.py:200:        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0917_Transformer_review.py:216:                    texts = self.tokenizer(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0917_Transformer_review.py:218:                    texts = clip.tokenize(texts).cuda()
cat_seg/modeling/transformer/ppg_0816_reshape.py:49:    x = coords_ij[..., 0].to(torch.float32) * (Wimg / W)
cat_seg/modeling/transformer/ppg_0816_reshape.py:50:    y = coords_ij[..., 1].to(torch.float32) * (Himg / H)
cat_seg/modeling/transformer/ppg_0816_reshape.py:124:                        km.fit(pts.cpu().numpy()).labels_
cat_seg/modeling/transformer/ppg_0816_reshape.py:125:                    ).to(device)
cat_seg/modeling/transformer/ppg_0816_reshape.py:246:    coords = torch.stack([xx, yy], dim=-1).reshape(P, 2).to(dtype)          # (P,2), [x,y] float
cat_seg/modeling/transformer/ppg_0816_reshape.py:273:        onehot = F.one_hot(assign, num_classes=K).to(dtype)                   # (B,C,P,K)
cat_seg/modeling/transformer/ppg_0816_reshape.py:274:        onehot = onehot * valid[..., None].to(dtype)                          # 無効画素は0
cat_seg/modeling/transformer/ppg_0816_reshape.py:286:        num = (w @ coords.to(dtype)).reshape(B, C, K, 2)                         # (B,C,K,2)
cat_seg/modeling/transformer/ppg_0816_reshape.py:297:    onehot = F.one_hot(assign, num_classes=K).to(dtype)                       # (B,C,P,K)
cat_seg/modeling/transformer/ppg_0816_reshape.py:298:    onehot = onehot * valid[..., None].to(dtype)
cat_seg/modeling/transformer/ppg_0816_reshape.py:316:    bin_masks = valid.reshape(B, C, H, W).to(dtype)
cat_seg/modeling/transformer/ppg_0816_reshape.py:321:        x = (coords_out[..., 0].to(torch.float32) * (Wimg / W)).round().long()
cat_seg/modeling/transformer/ppg_0816_reshape.py:322:        y = (coords_out[..., 1].to(torch.float32) * (Himg / H)).round().long()
cat_seg/modeling/transformer/model_0911_Wrapper_review.py:541:        self.sam_block = sam_block.build_sam_block().to(self.device)
cat_seg/modeling/transformer/model_0911_Wrapper_review.py:551:        self.conv1 = nn.Conv2d(prompt_channel, hidden_dim, kernel_size=1).to(self.device)
cat_seg/modeling/transformer/model_0911_Wrapper_review.py:592:        image_pe = self.sam_block.prompt_encoder.get_dense_pe().to(self.device)  # (1,256,?,?)
cat_seg/modeling/transformer/model_0911_Wrapper_review.py:603:            coords_b = coords[b].reshape(Nk, 1, 2).to(self.device)                  # (Nk,1,2)
cat_seg/modeling/transformer/model_0911_Wrapper_review.py:604:            labels_b = labels[b].reshape(Nk, 1).to(self.device)                     # (Nk,1)
cat_seg/modeling/transformer/model_0911_Wrapper_review.py:605:            masks_b  = region_masks[b].reshape(Nk, 1, H_emb_reg, W_emb_reg).to(self.device)
cat_seg/modeling/transformer/model_0917_Transformer_review.py:585:        self.sam_block = sam_block.build_sam_block().to(self.device)
cat_seg/modeling/transformer/model_0917_Transformer_review.py:595:        self.conv1 = nn.Conv2d(prompt_channel, hidden_dim, kernel_size=1).to(self.device)
cat_seg/modeling/transformer/model_0917_Transformer_review.py:600:        self.sam_prompts_conv = nn.Conv2d(self.k_pts*256, 256, kernel_size=1).to(self.device)
cat_seg/modeling/transformer/model_0917_Transformer_review.py:602:        self.c_conv = nn.Conv2d(in_channels=hidden_dim, out_channels=1, kernel_size=1, bias=True).to(self.device)
cat_seg/modeling/transformer/model_0917_Transformer_review.py:642:        image_pe = self.sam_block.prompt_encoder.get_dense_pe().to(self.device)  # (1,256,?,?)
cat_seg/modeling/transformer/model_0917_Transformer_review.py:653:            coords_b = coords[b].reshape(Nk, 1, 2).to(self.device)                  # (Nk,1,2)
cat_seg/modeling/transformer/model_0917_Transformer_review.py:654:            labels_b = labels[b].reshape(Nk, 1).to(self.device)                     # (Nk,1)
cat_seg/modeling/transformer/model_0917_Transformer_review.py:655:            masks_b  = region_masks[b].reshape(Nk, 1, H_emb_reg, W_emb_reg).to(self.device)
cat_seg/modeling/transformer/model_0909_ppg_sam.py:539:        self.sam_block = sam_block.build_sam_block().to(self.device)
cat_seg/modeling/transformer/model_0909_ppg_sam.py:583:        image_pe = self.sam_block.prompt_encoder.get_dense_pe().to(self.device)  # (1,256,?,?)
cat_seg/modeling/transformer/model_0909_ppg_sam.py:594:            coords_b = coords[b].reshape(Nk, 1, 2).to(self.device)                  # (Nk,1,2)
cat_seg/modeling/transformer/model_0909_ppg_sam.py:595:            labels_b = labels[b].reshape(Nk, 1).to(self.device)                     # (Nk,1)
cat_seg/modeling/transformer/model_0909_ppg_sam.py:596:            masks_b  = region_masks[b].reshape(Nk, 1, H_emb_reg, W_emb_reg).to(self.device)
cat_seg/modeling/transformer/cat_seg_predictor_0910.py:187:                texts = self.tokenizer(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0910.py:189:                texts = clip.tokenize(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0910.py:197:        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0910.py:213:                    texts = self.tokenizer(texts).cuda()
cat_seg/modeling/transformer/cat_seg_predictor_0910.py:215:                    texts = clip.tokenize(texts).cuda()
segment-anything/segment_anything/utils/transforms.py:78:        coords = deepcopy(coords).to(torch.float)
segment-anything/segment_anything/automatic_mask_generator.py:213:            scores = scores.to(data["boxes"].device)
segment-anything/segment_anything/predictor.py:163:        masks_np = masks[0].detach().cpu().numpy()
segment-anything/segment_anything/predictor.py:164:        iou_predictions_np = iou_predictions[0].detach().cpu().numpy()
segment-anything/segment_anything/predictor.py:165:        low_res_masks_np = low_res_masks[0].detach().cpu().numpy()
segment-anything/segment_anything/utils/amg.py:51:                self._stats[k] = v[keep.detach().cpu().numpy()]
segment-anything/segment_anything/utils/amg.py:75:                self._stats[k] = v.detach().cpu().numpy()
segment-anything/segment_anything/utils/amg.py:133:        counts.extend(btw_idxs.detach().cpu().tolist())
segment-anything/segment_anything/utils/onnx.py:45:        input_image_size = input_image_size.to(torch.float32)
segment-anything/segment_anything/utils/onnx.py:48:        transformed_size = torch.floor(transformed_size + 0.5).to(torch.int64)
segment-anything/segment_anything/utils/onnx.py:84:        prepadded_size = self.resize_longest_image_size(orig_im_size, self.img_size).to(torch.int64)
segment-anything/segment_anything/utils/onnx.py:87:        orig_im_size = orig_im_size.to(torch.int64)
segment-anything/segment_anything/utils/onnx.py:99:        ).to(iou_preds.device)
segment-anything/segment_anything/modeling/prompt_encoder.py:214:        return self._pe_encoding(coords.to(torch.float))  # B x N x C
